{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# 保险智能问答\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## 1、项目背景\n",
    "\n",
    "智能问答是获取信息和知识的更直接、更高效的方式之一，传统的信息检索方法智能找到相关的文档，而智能问答能够直接找到精准的答案，极大的节省了人们查询信息的时间。问答按照技术分为基于阅读理解的问答和检索式的问答，阅读理解的问答是在正文中找到对应的答案片段，检索式问答则是匹配高频的问题，然后把答案返回给用户。本项目属于检索式的问答，问答的领域用途很广，比如搜索引擎，小度音响等智能硬件，政府，金融，银行，电信，电商领域的智能客服，聊天机器人等。下图是保险领域的问答示例：\n",
    "\n",
    "\n",
    "![](https://ai-studio-static-online.cdn.bcebos.com/501cb43b339f4ba281836732151a15098a3e902c7d374acab86c695c4c6f8b7a)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### 1.1 应用特色\n",
    "\n",
    "+ 低门槛\n",
    "\n",
    "    + 手把手搭建检索式 FAQ System\n",
    "    + 无需相似 Query-Query Pair 标注数据也能构建 FAQ System\n",
    "+ 效果好\n",
    "\n",
    "    + 业界领先的检索预训练模型: RocketQA DualEncoder\n",
    "    + 针对无标注数据场景的领先解决方案: 检索预训练模型 + 增强的无监督语义索引微调\n",
    "+ 性能快\n",
    "\n",
    "    + 基于 Paddle Inference 快速抽取向量\n",
    "    + 基于 Milvus 快速查询和高性能建库\n",
    "    + 基于 Paddle Serving 高性能部署\n",
    "    \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 问答流程设计\n",
    "\n",
    "\n",
    "<center>\n",
    "<img src=\"https://ai-studio-static-online.cdn.bcebos.com/ec660d96b014404c8c717f1edeacb604f7abfc85685341be849be77b2b5615cd\" width=\"500px\"  ></center>\n",
    "\n",
    "\n",
    "问答的流程分为两部分，第一部分是管理员/工程师流程，第二部分就是用户使用流程，在模型的层面，需要离线的准备数据集，训练模型，然后把训练好的模型部署上线。另外，就是线上搭建问答检索引擎，第一步把收集好的语料数据，利用训练好的模型抽取问题的向量，然后把向量插入到近似向量检索引擎中，构建语义索引库，这部分做完了之后，就可以使用这个问答服务了，但是用户输入了Query之后，发生了什么呢？第一步就是线上服务会接收Query后，对数据进行处理，并抽取用户Query的向量，然后在ANN查询模块进行检索匹配相近的问题，最终选取Top10条数据，返回给线上服务，线上服务经过一定的处理，把最终的答案呈现给用户。\n",
    "\n",
    "本次模型的优化流程如下：\n",
    "\n",
    "![](https://ai-studio-static-online.cdn.bcebos.com/1c8bae0b83174318b2facd9e03b26e20e0ab3a7b3d0742a5a5b20bc3804246b6)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2、安装说明\n",
    "\n",
    "AI Studio平台默认安装了Paddle和PaddleNLP，并定期更新版本。 如需手动更新，可参考如下说明："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-18T06:30:21.743179Z",
     "iopub.status.busy": "2022-07-18T06:30:21.742643Z",
     "iopub.status.idle": "2022-07-18T06:30:45.503731Z",
     "shell.execute_reply": "2022-07-18T06:30:45.502661Z",
     "shell.execute_reply.started": "2022-07-18T06:30:21.743147Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.7.13 ('my_paddlenlp')' requires ipykernel package.\n",
      "Run the following command to install 'ipykernel' into the Python environment. \n",
      "Command: 'conda install -n my_paddlenlp ipykernel --update-deps --force-reinstall'"
     ]
    }
   ],
   "source": [
    "# 首次更新完以后，重启后方能生效\n",
    "!pip install --upgrade paddlenlp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "安装项目依赖的其他库：\n",
    "\n",
    "备注：如果提示找不到相关文件，左上角刷新即可。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-18T06:31:38.400769Z",
     "iopub.status.busy": "2022-07-18T06:31:38.399790Z",
     "iopub.status.idle": "2022-07-18T06:32:59.647837Z",
     "shell.execute_reply": "2022-07-18T06:32:59.646630Z",
     "shell.execute_reply.started": "2022-07-18T06:31:38.400734Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple/\n",
      "Requirement already satisfied: paddle-serving-app>=0.7.0 in /home/wcf/anaconda3/lib/python3.8/site-packages (from -r requirements.txt (line 1)) (0.9.0)\n",
      "Requirement already satisfied: paddle-serving-client>=0.7.0 in /home/wcf/anaconda3/lib/python3.8/site-packages (from -r requirements.txt (line 2)) (0.9.0)\n",
      "Requirement already satisfied: paddle-serving-server-gpu>=0.7.0.post102 in /home/wcf/anaconda3/lib/python3.8/site-packages (from -r requirements.txt (line 3)) (0.9.0.post1028)\n",
      "Requirement already satisfied: hnswlib>=0.5.2 in /home/wcf/anaconda3/lib/python3.8/site-packages (from -r requirements.txt (line 4)) (0.6.2)\n",
      "Requirement already satisfied: numpy in /home/wcf/anaconda3/lib/python3.8/site-packages (from hnswlib>=0.5.2->-r requirements.txt (line 4)) (1.20.1)\n",
      "Requirement already satisfied: shapely in /home/wcf/anaconda3/lib/python3.8/site-packages (from paddle-serving-app>=0.7.0->-r requirements.txt (line 1)) (1.8.2)\n",
      "Requirement already satisfied: opencv-python==3.4.17.61 in /home/wcf/anaconda3/lib/python3.8/site-packages (from paddle-serving-app>=0.7.0->-r requirements.txt (line 1)) (3.4.17.61)\n",
      "Requirement already satisfied: pyclipper in /home/wcf/anaconda3/lib/python3.8/site-packages (from paddle-serving-app>=0.7.0->-r requirements.txt (line 1)) (1.3.0.post3)\n",
      "Requirement already satisfied: pillow in /home/wcf/anaconda3/lib/python3.8/site-packages (from paddle-serving-app>=0.7.0->-r requirements.txt (line 1)) (8.2.0)\n",
      "Requirement already satisfied: sentencepiece<=0.1.96 in /home/wcf/anaconda3/lib/python3.8/site-packages (from paddle-serving-app>=0.7.0->-r requirements.txt (line 1)) (0.1.96)\n",
      "Requirement already satisfied: six>=1.10.0 in /home/wcf/anaconda3/lib/python3.8/site-packages (from paddle-serving-app>=0.7.0->-r requirements.txt (line 1)) (1.15.0)\n",
      "Requirement already satisfied: grpcio-tools<=1.33.2 in /home/wcf/anaconda3/lib/python3.8/site-packages (from paddle-serving-client>=0.7.0->-r requirements.txt (line 2)) (1.33.2)\n",
      "Requirement already satisfied: protobuf>=3.11.0 in /home/wcf/anaconda3/lib/python3.8/site-packages (from paddle-serving-client>=0.7.0->-r requirements.txt (line 2)) (3.20.0)\n",
      "Requirement already satisfied: grpcio<=1.33.2 in /home/wcf/anaconda3/lib/python3.8/site-packages (from paddle-serving-client>=0.7.0->-r requirements.txt (line 2)) (1.33.2)\n",
      "Requirement already satisfied: requests in /home/wcf/anaconda3/lib/python3.8/site-packages (from paddle-serving-client>=0.7.0->-r requirements.txt (line 2)) (2.25.1)\n",
      "Requirement already satisfied: Werkzeug==1.0.1 in /home/wcf/anaconda3/lib/python3.8/site-packages (from paddle-serving-server-gpu>=0.7.0.post102->-r requirements.txt (line 3)) (1.0.1)\n",
      "Requirement already satisfied: pyyaml in /home/wcf/anaconda3/lib/python3.8/site-packages (from paddle-serving-server-gpu>=0.7.0.post102->-r requirements.txt (line 3)) (5.4.1)\n",
      "Requirement already satisfied: func-timeout in /home/wcf/anaconda3/lib/python3.8/site-packages (from paddle-serving-server-gpu>=0.7.0.post102->-r requirements.txt (line 3)) (4.3.5)\n",
      "Requirement already satisfied: flask<2.0.0,>=1.1.1 in /home/wcf/anaconda3/lib/python3.8/site-packages (from paddle-serving-server-gpu>=0.7.0.post102->-r requirements.txt (line 3)) (1.1.2)\n",
      "Requirement already satisfied: pytest in /home/wcf/anaconda3/lib/python3.8/site-packages (from paddle-serving-server-gpu>=0.7.0.post102->-r requirements.txt (line 3)) (6.2.3)\n",
      "Requirement already satisfied: MarkupSafe==1.1.1 in /home/wcf/anaconda3/lib/python3.8/site-packages (from paddle-serving-server-gpu>=0.7.0.post102->-r requirements.txt (line 3)) (1.1.1)\n",
      "Requirement already satisfied: click==7.1.2 in /home/wcf/anaconda3/lib/python3.8/site-packages (from paddle-serving-server-gpu>=0.7.0.post102->-r requirements.txt (line 3)) (7.1.2)\n",
      "Requirement already satisfied: itsdangerous==1.1.0 in /home/wcf/anaconda3/lib/python3.8/site-packages (from paddle-serving-server-gpu>=0.7.0.post102->-r requirements.txt (line 3)) (1.1.0)\n",
      "Requirement already satisfied: Jinja2==2.11.3 in /home/wcf/anaconda3/lib/python3.8/site-packages (from paddle-serving-server-gpu>=0.7.0.post102->-r requirements.txt (line 3)) (2.11.3)\n",
      "Requirement already satisfied: attrs>=19.2.0 in /home/wcf/anaconda3/lib/python3.8/site-packages (from pytest->paddle-serving-server-gpu>=0.7.0.post102->-r requirements.txt (line 3)) (20.3.0)\n",
      "Requirement already satisfied: iniconfig in /home/wcf/anaconda3/lib/python3.8/site-packages (from pytest->paddle-serving-server-gpu>=0.7.0.post102->-r requirements.txt (line 3)) (1.1.1)\n",
      "Requirement already satisfied: packaging in /home/wcf/anaconda3/lib/python3.8/site-packages (from pytest->paddle-serving-server-gpu>=0.7.0.post102->-r requirements.txt (line 3)) (20.9)\n",
      "Requirement already satisfied: pluggy<1.0.0a1,>=0.12 in /home/wcf/anaconda3/lib/python3.8/site-packages (from pytest->paddle-serving-server-gpu>=0.7.0.post102->-r requirements.txt (line 3)) (0.13.1)\n",
      "Requirement already satisfied: py>=1.8.2 in /home/wcf/anaconda3/lib/python3.8/site-packages (from pytest->paddle-serving-server-gpu>=0.7.0.post102->-r requirements.txt (line 3)) (1.10.0)\n",
      "Requirement already satisfied: toml in /home/wcf/anaconda3/lib/python3.8/site-packages (from pytest->paddle-serving-server-gpu>=0.7.0.post102->-r requirements.txt (line 3)) (0.10.2)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /home/wcf/anaconda3/lib/python3.8/site-packages (from packaging->pytest->paddle-serving-server-gpu>=0.7.0.post102->-r requirements.txt (line 3)) (2.4.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/wcf/anaconda3/lib/python3.8/site-packages (from requests->paddle-serving-client>=0.7.0->-r requirements.txt (line 2)) (2020.12.5)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /home/wcf/anaconda3/lib/python3.8/site-packages (from requests->paddle-serving-client>=0.7.0->-r requirements.txt (line 2)) (2.10)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/wcf/anaconda3/lib/python3.8/site-packages (from requests->paddle-serving-client>=0.7.0->-r requirements.txt (line 2)) (1.26.4)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /home/wcf/anaconda3/lib/python3.8/site-packages (from requests->paddle-serving-client>=0.7.0->-r requirements.txt (line 2)) (4.0.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install -r requirements.txt  -i https://pypi.tuna.tsinghua.edu.cn/simple/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "首先导入项目所需要的第三方库："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-18T06:34:41.834218Z",
     "iopub.status.busy": "2022-07-18T06:34:41.833428Z",
     "iopub.status.idle": "2022-07-18T06:34:44.476737Z",
     "shell.execute_reply": "2022-07-18T06:34:44.475648Z",
     "shell.execute_reply.started": "2022-07-18T06:34:41.834180Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 加载系统的API\n",
    "import abc\n",
    "import sys\n",
    "from functools import partial\n",
    "import argparse\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "import numpy as np\n",
    "# 加载飞桨的API\n",
    "import paddle\n",
    "import paddle.nn as nn\n",
    "import paddle.nn.functional as F\n",
    "from paddle import inference\n",
    "# 加载PaddleNLP的API\n",
    "import paddlenlp as ppnlp\n",
    "from paddlenlp.data import Stack, Tuple, Pad\n",
    "from paddlenlp.datasets import load_dataset, MapDataset\n",
    "from paddlenlp.transformers import LinearDecayWithWarmup\n",
    "from paddlenlp.utils.downloader import get_path_from_url\n",
    "# import paddle_serving_client.io as serving_io\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3、数据准备\n",
    "\n",
    "基于Github上公开的保险问答数据集，我们需要准备训练集，评估集和召回库三部分。首先保险的数据包含Query，Title，Reply等字段，我们选取其中的Query和Reply字段来构建问答系统。\n",
    "\n",
    "Github的地址为：[保险](https://github.com/SophonPlus/ChineseNlpCorpus/blob/master/datasets/baoxianzhidao/intro.ipynb)\n",
    "\n",
    "**【免责声明】：数据集是基于Github开源数据进行了处理得到的，如果有任何侵权问题，请及时联系，我们会在第一时间进行删除处理。**\n",
    "\n",
    "\n",
    "有需要的同学去改地址下载数据即可。\n",
    "\n",
    "接下来我们构造训练集，训练集则直接使用保险数据中的Query，然后为了优化效果，我们使用同义词替换的方法构造同义句，构造的同义句如下：\n",
    "训练集示例如下：\n",
    "\n",
    "```\n",
    "我儿子还在念小学，适不适合投保康惠保呢？\n",
    "被骄车撞成右膀肱骨粉碎性骨折保守治疗怎么向车主和他的保险公司索赔\n",
    "商业医疗保险报销程序?\n",
    "家里有社保，还有必要买重疾险吗？\n",
    "工地买了建工险，出了事故多长时间上报保险公司有效\n",
    "请问下哆啦a保值不值得买呢？不晓得保障多不多\n",
    "```\n",
    "利用同义词替换的方法生成的训练集如下：\n",
    "\n",
    "```\n",
    "我儿子还在念小学，适不适合投保康惠保呢？\t我儿子还在念小学校，适不适合投保康惠保呢？\n",
    "被骄车撞成右膀肱骨粉碎性骨折保守治疗怎么向车主和他的保险公司索赔\t被骄车撞成右膀肱骨粉碎性骨折保守诊疗怎么向车主和他的保险公司索赔\n",
    "商业医疗保险报销程序?\t买卖医疗保险报销程序?\n",
    "家里有社保，还有必要买重疾险吗？\t家里有社保，再有必要买重疾险吗？\n",
    "工地买了建工险，出了事故多长时间上报保险公司有效\t工地买了建工险，出了事故多长时间上报保险公司管事\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "用中英文回译的方法来生成评估集合，评估集是问题对，示例如下：\n",
    "\n",
    "```\n",
    "企业养老保险自己怎么办理\t如何办理企业养老保险\n",
    "西*牙签证保险怎么买？\t如何为西班牙购买签证保险？\n",
    "康惠保的保额要买到多少才合适？\t康慧宝需要买多少？\n",
    "车辆事故对方全责维修费不肯垫付怎么办\t如果另一方对车辆事故负有全部责任，并且拒绝提前支付维修费，该怎么办\n",
    "准备清明节去新*坡旅行，哪款旅游险好？\t准备清明节去新兴坡旅游，什么样的旅游保险好？\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 加载数据\n",
    "\n",
    "加载数据集，可以选择`train.csv`或者`train_aug.csv`，`train.csv`表示的是无监督数据集，`train_aug.csv`表示的是同义词替换的数据集，可以二选一。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-18T06:42:08.311914Z",
     "iopub.status.busy": "2022-07-18T06:42:08.310731Z",
     "iopub.status.idle": "2022-07-18T06:42:08.328372Z",
     "shell.execute_reply": "2022-07-18T06:42:08.327624Z",
     "shell.execute_reply.started": "2022-07-18T06:42:08.311861Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text_a': '我儿子还在念小学，适不适合投保康惠保呢？', 'text_b': '我儿子还在念小学校，适不适合投保康惠保呢？'}\n",
      "{'text_a': '被骄车撞成右膀肱骨粉碎性骨折保守治疗怎么向车主和他的保险公司索赔', 'text_b': '被骄车撞成右膀肱骨粉碎性骨折保守诊疗怎么向车主和他的保险公司索赔'}\n",
      "{'text_a': '商业医疗保险报销程序?', 'text_b': '买卖医疗保险报销程序?'}\n"
     ]
    }
   ],
   "source": [
    "from data import read_text_pair\n",
    "\n",
    "def read_simcse_text(data_path):\n",
    "    \"\"\"Reads data.\"\"\"\n",
    "    with open(data_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            data = line.rstrip()\n",
    "            # 无监督训练，text_a和text_b是一样的\n",
    "            yield {'text_a': data, 'text_b': data}\n",
    "# 加载训练集， 无监督\n",
    "# train_set_file='databaoxian/train.csv'\n",
    "# train_ds = load_dataset(read_simcse_text, data_path=train_set_file, lazy=False)\n",
    "\n",
    "# 加载数据集，数据增强：\n",
    "train_set_file='databaoxian/train_aug.csv'\n",
    "train_ds = load_dataset(read_text_pair, data_path=train_set_file, lazy=False)\n",
    "\n",
    "# 输出三条数据\n",
    "for i  in range(3):\n",
    "    print(train_ds[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "打印结果可以看出，无监督数据：输入数据的两条文本是一样的。对于增强后的数据，两条文本是不一样的，可以比较一下差别。无监督数据读取使用`read_simcse_text`，增强的数据读取使用`read_text_pair`。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 构建Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-18T06:42:31.170822Z",
     "iopub.status.busy": "2022-07-18T06:42:31.169609Z",
     "iopub.status.idle": "2022-07-18T06:42:31.412319Z",
     "shell.execute_reply": "2022-07-18T06:42:31.411575Z",
     "shell.execute_reply.started": "2022-07-18T06:42:31.170766Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2022-07-18 15:37:32,200] [    INFO]\u001b[0m - Downloading https://bj.bcebos.com/paddlenlp/models/transformers/rocketqa/rocketqa-zh-dureader-vocab.txt and saved to /home/wcf/.paddlenlp/models/rocketqa-zh-dureader-query-encoder\u001b[0m\n",
      "\u001b[32m[2022-07-18 15:37:32,210] [    INFO]\u001b[0m - Downloading rocketqa-zh-dureader-vocab.txt from https://bj.bcebos.com/paddlenlp/models/transformers/rocketqa/rocketqa-zh-dureader-vocab.txt\u001b[0m\n",
      "100%|██████████| 89.0k/89.0k [00:00<00:00, 554kB/s]\n",
      "\u001b[32m[2022-07-18 15:37:32,674] [    INFO]\u001b[0m - tokenizer config file saved in /home/wcf/.paddlenlp/models/rocketqa-zh-dureader-query-encoder/tokenizer_config.json\u001b[0m\n",
      "\u001b[32m[2022-07-18 15:37:32,675] [    INFO]\u001b[0m - Special tokens file saved in /home/wcf/.paddlenlp/models/rocketqa-zh-dureader-query-encoder/special_tokens_map.json\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "def convert_example(example, tokenizer, max_seq_length=512, do_evalute=False):\n",
    "    # 把文本转换成id的形式\n",
    "    result = []\n",
    "\n",
    "    for key, text in example.items():\n",
    "        if 'label' in key:\n",
    "            # do_evaluate\n",
    "            result += [example['label']]\n",
    "        else:\n",
    "            # do_train\n",
    "            encoded_inputs = tokenizer(text=text, max_seq_len=max_seq_length)\n",
    "            input_ids = encoded_inputs[\"input_ids\"]\n",
    "            token_type_ids = encoded_inputs[\"token_type_ids\"]\n",
    "            result += [input_ids, token_type_ids]\n",
    "    return result\n",
    "\n",
    "\n",
    "# 序列的最大的长度，根据数据集的情况进行设置\n",
    "max_seq_length=64\n",
    "# batch_size越大，效果会更好\n",
    "batch_size=64\n",
    "# 使用rocketqa开放领域的问答模型\n",
    "model_name_or_path='rocketqa-zh-dureader-query-encoder'\n",
    "tokenizer = ppnlp.transformers.ErnieTokenizer.from_pretrained(model_name_or_path)\n",
    "# partial赋默认的值\n",
    "trans_func = partial(\n",
    "        convert_example,\n",
    "        tokenizer=tokenizer,\n",
    "        max_seq_length=max_seq_length)\n",
    "# 对齐组装成小批次数据\n",
    "batchify_fn = lambda samples, fn=Tuple(\n",
    "        Pad(axis=0, pad_val=tokenizer.pad_token_id),  # query_input\n",
    "        Pad(axis=0, pad_val=tokenizer.pad_token_type_id),  # query_segment\n",
    "        Pad(axis=0, pad_val=tokenizer.pad_token_id),  # title_input\n",
    "        Pad(axis=0, pad_val=tokenizer.pad_token_type_id),  # tilte_segment\n",
    "    ): [data for data in fn(samples)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-18T06:42:45.947267Z",
     "iopub.status.busy": "2022-07-18T06:42:45.946462Z",
     "iopub.status.idle": "2022-07-18T06:42:47.764779Z",
     "shell.execute_reply": "2022-07-18T06:42:47.763089Z",
     "shell.execute_reply.started": "2022-07-18T06:42:45.947230Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Tensor(shape=[64, 44], dtype=int64, place=Place(cpu), stop_gradient=True,\n",
      "       [[1   , 278 , 26  , ..., 0   , 0   , 0   ],\n",
      "        [1   , 1042, 15  , ..., 0   , 0   , 0   ],\n",
      "        [1   , 320 , 645 , ..., 0   , 0   , 0   ],\n",
      "        ...,\n",
      "        [1   , 68  , 124 , ..., 0   , 0   , 0   ],\n",
      "        [1   , 276 , 1023, ..., 0   , 0   , 0   ],\n",
      "        [1   , 118 , 19  , ..., 0   , 0   , 0   ]]), Tensor(shape=[64, 44], dtype=int64, place=Place(cpu), stop_gradient=True,\n",
      "       [[0, 0, 0, ..., 0, 0, 0],\n",
      "        [0, 0, 0, ..., 0, 0, 0],\n",
      "        [0, 0, 0, ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0, ..., 0, 0, 0],\n",
      "        [0, 0, 0, ..., 0, 0, 0],\n",
      "        [0, 0, 0, ..., 0, 0, 0]]), Tensor(shape=[64, 42], dtype=int64, place=Place(cpu), stop_gradient=True,\n",
      "       [[1   , 1042, 1225, ..., 0   , 0   , 0   ],\n",
      "        [1   , 1042, 15  , ..., 0   , 0   , 0   ],\n",
      "        [1   , 320 , 645 , ..., 0   , 0   , 0   ],\n",
      "        ...,\n",
      "        [1   , 68  , 124 , ..., 0   , 0   , 0   ],\n",
      "        [1   , 276 , 1023, ..., 0   , 0   , 0   ],\n",
      "        [1   , 118 , 19  , ..., 0   , 0   , 0   ]]), Tensor(shape=[64, 42], dtype=int64, place=Place(cpu), stop_gradient=True,\n",
      "       [[0, 0, 0, ..., 0, 0, 0],\n",
      "        [0, 0, 0, ..., 0, 0, 0],\n",
      "        [0, 0, 0, ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0, ..., 0, 0, 0],\n",
      "        [0, 0, 0, ..., 0, 0, 0],\n",
      "        [0, 0, 0, ..., 0, 0, 0]])]\n"
     ]
    }
   ],
   "source": [
    "# 明文数据 -> ID 序列训练数据\n",
    "def create_dataloader(dataset,\n",
    "                      mode='train',\n",
    "                      batch_size=1,\n",
    "                      batchify_fn=None,\n",
    "                      trans_fn=None):\n",
    "    if trans_fn:\n",
    "        dataset = dataset.map(trans_fn)\n",
    "\n",
    "    shuffle = True if mode == 'train' else False\n",
    "    if mode == 'train':\n",
    "        batch_sampler = paddle.io.DistributedBatchSampler(\n",
    "            dataset, batch_size=batch_size, shuffle=shuffle)\n",
    "    else:\n",
    "        batch_sampler = paddle.io.BatchSampler(\n",
    "            dataset, batch_size=batch_size, shuffle=shuffle)\n",
    "\n",
    "    return paddle.io.DataLoader(\n",
    "        dataset=dataset,\n",
    "        batch_sampler=batch_sampler,\n",
    "        collate_fn=batchify_fn,\n",
    "        return_list=True)\n",
    "\n",
    "# 构建训练的Dataloader\n",
    "train_data_loader = create_dataloader(\n",
    "        train_ds,\n",
    "        mode='train',\n",
    "        batch_size=batch_size,\n",
    "        batchify_fn=batchify_fn,\n",
    "        trans_fn=trans_func)\n",
    "# 展示一下输入的dataloader的数据\n",
    "for idx, batch in enumerate(train_data_loader):\n",
    "    if idx == 0:\n",
    "        print(batch)\n",
    "        break "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上面展示的是一个batch的数据，包含两个Tensor，第一个Tensor表示的是input_ids，第二个Tensor表示的是token_type_ids；第一个Tensor中，32是batch_size的维度，44代表的是序列的长度，表示输入的文本的最大长度是44；第二个Tensor中，32表示的也是batch_size，44表示的是序列的长度。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4、模型选择\n",
    "\n",
    "首先保险问答场景的数据只有问题和答案对，再没有其他的数据了。如果使用有监督方法，需要问题-问题对，还需要收集一些问题进行人工标注。因此可以考虑使用无监督语义索引技术SimCSE模型。\n",
    "\n",
    "\n",
    "总体上无监督技术没有有监督技术效果好，所以为了提升SimCSE的性能，我们使用了开放问答领域的预训练语言模型RocketQA，并且在SimCSE的基础上利用WR，R-Drop等策略进行优化。\n",
    "\n",
    "\n",
    "\n",
    "## 4.1 模型方案设计\n",
    "\n",
    "无监督方案\n",
    "\n",
    "第一步：基于检索式问答SOTA预训练模型RocketQA\n",
    "\n",
    "第二步：无监督训练策略SimCSE\n",
    "\n",
    "第三步：无监督增强策略Word Reptition, WR, RDrop\n",
    "\n",
    "<center>\n",
    "<img src=\"https://ai-studio-static-online.cdn.bcebos.com/cdfb127b542f41828da883ba7f83ca592fc15d3e81ce4f41bf9c76af6954c3a3\" width=\"500px\"  ></center>\n",
    "\n",
    "整个方案无需人工参与数据标注，所以是一个无监督的解决方案。\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5、模型构建\n",
    "\n",
    "\n",
    "## 5.1 SimCSE模型\n",
    "\n",
    "搭建SimCSE模型，主要部分是用query和title分别得到embedding向量，然后计算余弦相似度。\n",
    "\n",
    "![](https://ai-studio-static-online.cdn.bcebos.com/604dfebaee5e4f25b035551cb131afa8434efed390c847c3ad4f5fb5d1598d33)\n",
    "\n",
    "上图是SimCSE的原理图，SimCSE主要是通过dropout来把同一个句子变成正样本（做两次前向，但是dropout有随机因素，所以产生的向量不一样，但是本质上还是表示的是同一句话），把一个batch里面其他的句子变成负样本的。\n",
    "\n",
    "SimCSE网络结构搭建，搭建代码如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-18T06:43:17.247229Z",
     "iopub.status.busy": "2022-07-18T06:43:17.246338Z",
     "iopub.status.idle": "2022-07-18T06:43:17.265308Z",
     "shell.execute_reply": "2022-07-18T06:43:17.264337Z",
     "shell.execute_reply.started": "2022-07-18T06:43:17.247194Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "class SimCSE(nn.Layer):\n",
    "    def __init__(self,\n",
    "                 pretrained_model,\n",
    "                 dropout=None,\n",
    "                 margin=0.0,\n",
    "                 scale=20,\n",
    "                 output_emb_size=None):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.ptm = pretrained_model\n",
    "        # 显式的加一个dropout来控制\n",
    "        self.dropout = nn.Dropout(dropout if dropout is not None else 0.1)\n",
    "\n",
    "        # 考虑到性能和效率，我们推荐把output_emb_size设置成256\n",
    "        # 向量越大，语义信息越丰富，但消耗资源越多\n",
    "        self.output_emb_size = output_emb_size\n",
    "        if output_emb_size > 0:\n",
    "            weight_attr = paddle.ParamAttr(\n",
    "                initializer=paddle.nn.initializer.TruncatedNormal(std=0.02))\n",
    "            self.emb_reduce_linear = paddle.nn.Linear(\n",
    "                768, output_emb_size, weight_attr=weight_attr)\n",
    "\n",
    "        self.margin = margin\n",
    "        \n",
    "        # 为了使余弦相似度更容易收敛，我们选择把计算出来的余弦相似度扩大scale倍，一般设置成20左右\n",
    "        self.sacle = scale\n",
    "        # 二分类计算\n",
    "        self.classifier = nn.Linear(output_emb_size, 2)\n",
    "        # R-Drop的损失\n",
    "        self.rdrop_loss = ppnlp.losses.RDropLoss()\n",
    "\n",
    "    # 加入jit注释能够把该提取向量的函数导出成静态图\n",
    "    # 对应input_id,token_type_id两个\n",
    "    @paddle.jit.to_static(input_spec=[\n",
    "        paddle.static.InputSpec(\n",
    "            shape=[None, None], dtype='int64'), paddle.static.InputSpec(\n",
    "                shape=[None, None], dtype='int64')\n",
    "    ])\n",
    "    def get_pooled_embedding(self,\n",
    "                             input_ids,\n",
    "                             token_type_ids=None,\n",
    "                             position_ids=None,\n",
    "                             attention_mask=None,\n",
    "                             with_pooler=True):\n",
    "\n",
    "        # Note: cls_embedding is poolerd embedding with act tanh \n",
    "        sequence_output, cls_embedding = self.ptm(input_ids, token_type_ids,\n",
    "                                                  position_ids, attention_mask)\n",
    "\n",
    "        if with_pooler == False:\n",
    "            cls_embedding = sequence_output[:, 0, :]\n",
    "\n",
    "        if self.output_emb_size > 0:\n",
    "            cls_embedding = self.emb_reduce_linear(cls_embedding)\n",
    "\n",
    "        cls_embedding = self.dropout(cls_embedding)\n",
    "        cls_embedding = F.normalize(cls_embedding, p=2, axis=-1)\n",
    "\n",
    "        return cls_embedding\n",
    "\n",
    "    def get_semantic_embedding(self, data_loader):\n",
    "        self.eval()\n",
    "        with paddle.no_grad():\n",
    "            for batch_data in data_loader:\n",
    "                input_ids, token_type_ids = batch_data\n",
    "                input_ids = paddle.to_tensor(input_ids)\n",
    "                token_type_ids = paddle.to_tensor(token_type_ids)\n",
    "\n",
    "                text_embeddings = self.get_pooled_embedding(\n",
    "                    input_ids, token_type_ids=token_type_ids)\n",
    "\n",
    "                yield text_embeddings\n",
    "\n",
    "    def cosine_sim(self,\n",
    "                   query_input_ids,\n",
    "                   title_input_ids,\n",
    "                   query_token_type_ids=None,\n",
    "                   query_position_ids=None,\n",
    "                   query_attention_mask=None,\n",
    "                   title_token_type_ids=None,\n",
    "                   title_position_ids=None,\n",
    "                   title_attention_mask=None,\n",
    "                   with_pooler=True):\n",
    "\n",
    "        query_cls_embedding = self.get_pooled_embedding(\n",
    "            query_input_ids,\n",
    "            query_token_type_ids,\n",
    "            query_position_ids,\n",
    "            query_attention_mask,\n",
    "            with_pooler=with_pooler)\n",
    "\n",
    "        title_cls_embedding = self.get_pooled_embedding(\n",
    "            title_input_ids,\n",
    "            title_token_type_ids,\n",
    "            title_position_ids,\n",
    "            title_attention_mask,\n",
    "            with_pooler=with_pooler)\n",
    "\n",
    "        cosine_sim = paddle.sum(query_cls_embedding * title_cls_embedding,\n",
    "                                axis=-1)\n",
    "        return cosine_sim\n",
    "\n",
    "    def forward(self,\n",
    "                query_input_ids,\n",
    "                title_input_ids,\n",
    "                query_token_type_ids=None,\n",
    "                query_position_ids=None,\n",
    "                query_attention_mask=None,\n",
    "                title_token_type_ids=None,\n",
    "                title_position_ids=None,\n",
    "                title_attention_mask=None):\n",
    "        # 第 1 次编码: 文本经过无监督语义索引模型编码后的语义向量 \n",
    "        # [N, output_emb_size]\n",
    "        query_cls_embedding = self.get_pooled_embedding(\n",
    "            query_input_ids, query_token_type_ids, query_position_ids,\n",
    "            query_attention_mask)\n",
    "\n",
    "        # 第 2 次编码: 文本经过无监督语义索引模型编码后的语义向量 \n",
    "        # [N, output_emb_size]\n",
    "        title_cls_embedding = self.get_pooled_embedding(\n",
    "            title_input_ids, title_token_type_ids, title_position_ids,\n",
    "            title_attention_mask)\n",
    "\n",
    "        # 使用R-Drop\n",
    "        logits1=self.classifier(query_cls_embedding)\n",
    "        logits2 = self.classifier(title_cls_embedding)\n",
    "        kl_loss = self.rdrop_loss(logits1, logits2)\n",
    "\n",
    "        # 相似度矩阵: [N, N]\n",
    "        cosine_sim = paddle.matmul(\n",
    "            query_cls_embedding, title_cls_embedding, transpose_y=True)\n",
    "\n",
    "        # substract margin from all positive samples cosine_sim()\n",
    "        # 填充self.margin值，比如margin为0.2，query_cls_embedding.shape[0]=2 \n",
    "        # margin_diag: [0.2,0.2]\n",
    "        margin_diag = paddle.full(\n",
    "            shape=[query_cls_embedding.shape[0]],\n",
    "            fill_value=self.margin,\n",
    "            dtype=paddle.get_default_dtype())\n",
    "        # input paddle.diag(margin_diag): [[0.2,0],[0,0.2]]\n",
    "        # input cosine_sim : [[1.0,0.6],[0.6,1.0]]\n",
    "        # output cosine_sim: [[0.8,0.6],[0.6,0.8]]\n",
    "        cosine_sim = cosine_sim - paddle.diag(margin_diag)\n",
    "\n",
    "        # scale cosine to ease training converge\n",
    "        cosine_sim *= self.sacle\n",
    "        # 转化成分类任务: 对角线元素是正例，其余元素为负例\n",
    "        # labels : [0,1,2,3]\n",
    "        labels = paddle.arange(0, query_cls_embedding.shape[0], dtype='int64')\n",
    "        # labels : [[0],[1],[2],[3]]\n",
    "        labels = paddle.reshape(labels, shape=[-1, 1])\n",
    "        # 交叉熵损失函数\n",
    "        loss = F.cross_entropy(input=cosine_sim, label=labels)\n",
    "\n",
    "        return loss, kl_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 模型优化策略\n",
    "\n",
    "### 5.2.1 WR 策略\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "| 策略 | 举例 | 解释 |\n",
    "| -------- | -------- | -------- |\n",
    "| 原句     | 企业养老保险自己怎么办理     | -     |\n",
    "| WR策略（Yes）     | 企业养老老保险自己怎么么办理     | 语义改变较小     |\n",
    "| 随机插入(No)    | 无企业养老保险自己怎么办理     | 语义改变较大     |\n",
    "| 随机删除（No）     | 企业养保险自己怎么办理     | 语义改变较大     |\n",
    "\n",
    "\n",
    "上图是WR策略跟其他策略的简单比较，其中WR策略对原句的语义改变很小，但是改变了句子的长度，破除了SimCSE句子长度相等的假设。WR策略起源于ESimCSE的论文，有兴趣可以从论文里了解其原理。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-18T06:43:26.601831Z",
     "iopub.status.busy": "2022-07-18T06:43:26.601259Z",
     "iopub.status.idle": "2022-07-18T06:43:26.611150Z",
     "shell.execute_reply": "2022-07-18T06:43:26.610388Z",
     "shell.execute_reply.started": "2022-07-18T06:43:26.601794Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "def word_repetition(input_ids, token_type_ids, dup_rate=0.32):\n",
    "    \"\"\"Word Reptition strategy.\"\"\"\n",
    "    input_ids = input_ids.numpy().tolist()\n",
    "    token_type_ids = token_type_ids.numpy().tolist()\n",
    "\n",
    "    batch_size, seq_len = len(input_ids), len(input_ids[0])\n",
    "    repetitied_input_ids = []\n",
    "    repetitied_token_type_ids = []\n",
    "    rep_seq_len = seq_len\n",
    "    for batch_id in range(batch_size):\n",
    "        cur_input_id = input_ids[batch_id]\n",
    "        actual_len = np.count_nonzero(cur_input_id)\n",
    "        dup_word_index = []\n",
    "        # If sequence length is less than 5, skip it\n",
    "        if (actual_len > 5):\n",
    "            dup_len = random.randint(a=0, b=max(2, int(dup_rate * actual_len)))\n",
    "            # Skip cls and sep position\n",
    "            dup_word_index = random.sample(\n",
    "                list(range(1, actual_len - 1)), k=dup_len)\n",
    "\n",
    "        r_input_id = []\n",
    "        r_token_type_id = []\n",
    "        for idx, word_id in enumerate(cur_input_id):\n",
    "            # Insert duplicate word\n",
    "            if idx in dup_word_index:\n",
    "                r_input_id.append(word_id)\n",
    "                r_token_type_id.append(token_type_ids[batch_id][idx])\n",
    "            r_input_id.append(word_id)\n",
    "            r_token_type_id.append(token_type_ids[batch_id][idx])\n",
    "        after_dup_len = len(r_input_id)\n",
    "        repetitied_input_ids.append(r_input_id)\n",
    "        repetitied_token_type_ids.append(r_token_type_id)\n",
    "\n",
    "        if after_dup_len > rep_seq_len:\n",
    "            rep_seq_len = after_dup_len\n",
    "    # Padding the data to the same length\n",
    "    for batch_id in range(batch_size):\n",
    "        after_dup_len = len(repetitied_input_ids[batch_id])\n",
    "        pad_len = rep_seq_len - after_dup_len\n",
    "        repetitied_input_ids[batch_id] += [0] * pad_len\n",
    "        repetitied_token_type_ids[batch_id] += [0] * pad_len\n",
    "\n",
    "    return paddle.to_tensor(repetitied_input_ids), paddle.to_tensor(\n",
    "        repetitied_token_type_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2.2 R-Drop策略\n",
    "\n",
    "R-Drop的原理：为了避免过拟合，通常会加入Dropout等较为成熟的正则话策略，即同样的输入，分别用模型预测两次，因为有Dropout的存在，所以会得到两个不同分布的输出，可以近似的看作两个不同的模型的网络的输出。我们用P1和P2表示模型输出的两个不同分布，R-Drop的目的就是在训练的过程中不断拉低这两个分布之间的KL散度。\n",
    "\n",
    "R-Drop的API请参考：[https://paddlenlp.readthedocs.io/zh/latest/source/paddlenlp.losses.rdrop.html](https://paddlenlp.readthedocs.io/zh/latest/source/paddlenlp.losses.rdrop.html)\n",
    "\n",
    "\n",
    "![](https://ai-studio-static-online.cdn.bcebos.com/9ebf40560f4747969fc22c4ab4d86eaf6f798316a398475392f4230fe11bfcd4)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6.训练配置\n",
    "\n",
    "训练配置包括一些超参数，优化器，模型实例化等设置。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-18T06:43:33.892961Z",
     "iopub.status.busy": "2022-07-18T06:43:33.892400Z",
     "iopub.status.idle": "2022-07-18T06:43:33.897624Z",
     "shell.execute_reply": "2022-07-18T06:43:33.896853Z",
     "shell.execute_reply.started": "2022-07-18T06:43:33.892926Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 关键参数\n",
    "scale=20 # 推荐值: 10 ~ 30\n",
    "margin=0.1 # 推荐值: 0.0 ~ 0.2\n",
    "\n",
    "epochs= 3 \n",
    "# 学习率设置\n",
    "learning_rate=5E-5\n",
    "warmup_proportion=0.0\n",
    "weight_decay=0.0\n",
    "save_steps=10\n",
    "\n",
    "# 可以根据实际情况进行设置\n",
    "output_emb_size=256\n",
    "dup_rate=0.3 # 建议设置在0~0.3之间\n",
    "save_dir='checkpoints'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-18T06:43:41.514886Z",
     "iopub.status.busy": "2022-07-18T06:43:41.513720Z",
     "iopub.status.idle": "2022-07-18T06:43:57.781436Z",
     "shell.execute_reply": "2022-07-18T06:43:57.780318Z",
     "shell.execute_reply.started": "2022-07-18T06:43:41.514830Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2022-07-18 15:39:17,502] [    INFO]\u001b[0m - Downloading https://bj.bcebos.com/paddlenlp/models/transformers/rocketqa/rocketqa_zh_dureader_query_encoder.pdparams and saved to /home/wcf/.paddlenlp/models/rocketqa-zh-dureader-query-encoder\u001b[0m\n",
      "\u001b[32m[2022-07-18 15:39:17,505] [    INFO]\u001b[0m - Downloading rocketqa_zh_dureader_query_encoder.pdparams from https://bj.bcebos.com/paddlenlp/models/transformers/rocketqa/rocketqa_zh_dureader_query_encoder.pdparams\u001b[0m\n",
      "100%|██████████| 379M/379M [00:20<00:00, 19.8MB/s] \n"
     ]
    }
   ],
   "source": [
    "# 使用预训练模型\n",
    "pretrained_model = ppnlp.transformers.ErnieModel.from_pretrained(model_name_or_path)\n",
    "# 无监督+R-Drop，类似于多任务学习\n",
    "model = SimCSE(\n",
    "        pretrained_model,\n",
    "        margin=margin,\n",
    "        scale=scale,\n",
    "        output_emb_size=output_emb_size)\n",
    "\n",
    "num_training_steps = len(train_data_loader) * epochs\n",
    "\n",
    "lr_scheduler = LinearDecayWithWarmup(learning_rate, num_training_steps,\n",
    "                                         warmup_proportion) \n",
    "\n",
    "# Generate parameter names needed to perform weight decay.\n",
    "# All bias and LayerNorm parameters are excluded.\n",
    "decay_params = [\n",
    "        p.name for n, p in model.named_parameters()\n",
    "        if not any(nd in n for nd in [\"bias\", \"norm\"])\n",
    "    ]\n",
    "# AdamW优化器\n",
    "optimizer = paddle.optimizer.AdamW(\n",
    "        learning_rate=lr_scheduler,\n",
    "        parameters=model.parameters(),\n",
    "        weight_decay=weight_decay,\n",
    "        apply_decay_param_fun=lambda x: x in decay_params)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. 模型训练\n",
    "\n",
    "训练过程是从train_data_loader中不断得到小批次的数据，然后送入模型预测得到损失，然后反向更新梯度，代码如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-18T06:44:12.406967Z",
     "iopub.status.busy": "2022-07-18T06:44:12.406425Z",
     "iopub.status.idle": "2022-07-18T06:45:11.032776Z",
     "shell.execute_reply": "2022-07-18T06:45:11.032011Z",
     "shell.execute_reply.started": "2022-07-18T06:44:12.406928Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "global step 5, epoch: 1, batch: 5, loss: 0.62206, speed: 0.11 step/s\n",
      "global step 10, epoch: 1, batch: 10, loss: 0.20830, speed: 0.10 step/s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2022-07-18 15:43:32,019] [    INFO]\u001b[0m - tokenizer config file saved in checkpoints/model_10/tokenizer_config.json\u001b[0m\n",
      "\u001b[32m[2022-07-18 15:43:32,021] [    INFO]\u001b[0m - Special tokens file saved in checkpoints/model_10/special_tokens_map.json\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "global step 15, epoch: 1, batch: 15, loss: 0.29178, speed: 0.11 step/s\n",
      "global step 20, epoch: 1, batch: 20, loss: 0.12056, speed: 0.11 step/s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2022-07-18 15:46:29,062] [    INFO]\u001b[0m - tokenizer config file saved in checkpoints/model_20/tokenizer_config.json\u001b[0m\n",
      "\u001b[32m[2022-07-18 15:46:29,063] [    INFO]\u001b[0m - Special tokens file saved in checkpoints/model_20/special_tokens_map.json\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "global step 25, epoch: 1, batch: 25, loss: 0.13968, speed: 0.10 step/s\n",
      "global step 30, epoch: 1, batch: 30, loss: 0.11402, speed: 0.09 step/s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2022-07-18 15:50:01,620] [    INFO]\u001b[0m - tokenizer config file saved in checkpoints/model_30/tokenizer_config.json\u001b[0m\n",
      "\u001b[32m[2022-07-18 15:50:01,622] [    INFO]\u001b[0m - Special tokens file saved in checkpoints/model_30/special_tokens_map.json\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "global step 35, epoch: 1, batch: 35, loss: 0.10120, speed: 0.09 step/s\n",
      "global step 40, epoch: 1, batch: 40, loss: 0.08444, speed: 0.10 step/s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2022-07-18 15:53:32,309] [    INFO]\u001b[0m - tokenizer config file saved in checkpoints/model_40/tokenizer_config.json\u001b[0m\n",
      "\u001b[32m[2022-07-18 15:53:32,310] [    INFO]\u001b[0m - Special tokens file saved in checkpoints/model_40/special_tokens_map.json\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "global step 45, epoch: 1, batch: 45, loss: 0.12122, speed: 0.09 step/s\n",
      "global step 50, epoch: 2, batch: 2, loss: 0.03363, speed: 0.11 step/s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2022-07-18 15:56:58,851] [    INFO]\u001b[0m - tokenizer config file saved in checkpoints/model_50/tokenizer_config.json\u001b[0m\n",
      "\u001b[32m[2022-07-18 15:56:58,852] [    INFO]\u001b[0m - Special tokens file saved in checkpoints/model_50/special_tokens_map.json\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "global step 55, epoch: 2, batch: 7, loss: 0.04666, speed: 0.10 step/s\n",
      "global step 60, epoch: 2, batch: 12, loss: 0.03572, speed: 0.09 step/s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2022-07-18 16:00:26,543] [    INFO]\u001b[0m - tokenizer config file saved in checkpoints/model_60/tokenizer_config.json\u001b[0m\n",
      "\u001b[32m[2022-07-18 16:00:26,545] [    INFO]\u001b[0m - Special tokens file saved in checkpoints/model_60/special_tokens_map.json\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "global step 65, epoch: 2, batch: 17, loss: 0.06123, speed: 0.10 step/s\n",
      "global step 70, epoch: 2, batch: 22, loss: 0.01675, speed: 0.10 step/s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2022-07-18 16:03:56,461] [    INFO]\u001b[0m - tokenizer config file saved in checkpoints/model_70/tokenizer_config.json\u001b[0m\n",
      "\u001b[32m[2022-07-18 16:03:56,463] [    INFO]\u001b[0m - Special tokens file saved in checkpoints/model_70/special_tokens_map.json\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "global step 75, epoch: 2, batch: 27, loss: 0.14322, speed: 0.10 step/s\n",
      "global step 80, epoch: 2, batch: 32, loss: 0.03388, speed: 0.09 step/s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2022-07-18 16:07:22,325] [    INFO]\u001b[0m - tokenizer config file saved in checkpoints/model_80/tokenizer_config.json\u001b[0m\n",
      "\u001b[32m[2022-07-18 16:07:22,326] [    INFO]\u001b[0m - Special tokens file saved in checkpoints/model_80/special_tokens_map.json\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "global step 85, epoch: 2, batch: 37, loss: 0.12475, speed: 0.09 step/s\n",
      "global step 90, epoch: 2, batch: 42, loss: 0.09018, speed: 0.09 step/s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2022-07-18 16:11:03,344] [    INFO]\u001b[0m - tokenizer config file saved in checkpoints/model_90/tokenizer_config.json\u001b[0m\n",
      "\u001b[32m[2022-07-18 16:11:03,346] [    INFO]\u001b[0m - Special tokens file saved in checkpoints/model_90/special_tokens_map.json\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "global step 95, epoch: 2, batch: 47, loss: 0.08742, speed: 0.11 step/s\n",
      "global step 100, epoch: 3, batch: 4, loss: 0.07259, speed: 0.12 step/s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2022-07-18 16:13:57,932] [    INFO]\u001b[0m - tokenizer config file saved in checkpoints/model_100/tokenizer_config.json\u001b[0m\n",
      "\u001b[32m[2022-07-18 16:13:57,934] [    INFO]\u001b[0m - Special tokens file saved in checkpoints/model_100/special_tokens_map.json\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "global step 105, epoch: 3, batch: 9, loss: 0.03881, speed: 0.10 step/s\n",
      "global step 110, epoch: 3, batch: 14, loss: 0.04414, speed: 0.09 step/s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2022-07-18 16:17:25,912] [    INFO]\u001b[0m - tokenizer config file saved in checkpoints/model_110/tokenizer_config.json\u001b[0m\n",
      "\u001b[32m[2022-07-18 16:17:25,914] [    INFO]\u001b[0m - Special tokens file saved in checkpoints/model_110/special_tokens_map.json\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "global step 115, epoch: 3, batch: 19, loss: 0.05093, speed: 0.11 step/s\n",
      "global step 120, epoch: 3, batch: 24, loss: 0.03036, speed: 0.11 step/s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2022-07-18 16:20:32,127] [    INFO]\u001b[0m - tokenizer config file saved in checkpoints/model_120/tokenizer_config.json\u001b[0m\n",
      "\u001b[32m[2022-07-18 16:20:32,129] [    INFO]\u001b[0m - Special tokens file saved in checkpoints/model_120/special_tokens_map.json\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "global step 125, epoch: 3, batch: 29, loss: 0.03839, speed: 0.10 step/s\n",
      "global step 130, epoch: 3, batch: 34, loss: 0.04556, speed: 0.09 step/s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2022-07-18 16:24:06,635] [    INFO]\u001b[0m - tokenizer config file saved in checkpoints/model_130/tokenizer_config.json\u001b[0m\n",
      "\u001b[32m[2022-07-18 16:24:06,637] [    INFO]\u001b[0m - Special tokens file saved in checkpoints/model_130/special_tokens_map.json\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "global step 135, epoch: 3, batch: 39, loss: 0.06152, speed: 0.09 step/s\n",
      "global step 140, epoch: 3, batch: 44, loss: 0.05824, speed: 0.10 step/s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2022-07-18 16:27:40,359] [    INFO]\u001b[0m - tokenizer config file saved in checkpoints/model_140/tokenizer_config.json\u001b[0m\n",
      "\u001b[32m[2022-07-18 16:27:40,361] [    INFO]\u001b[0m - Special tokens file saved in checkpoints/model_140/special_tokens_map.json\u001b[0m\n",
      "\u001b[32m[2022-07-18 16:28:50,555] [    INFO]\u001b[0m - tokenizer config file saved in checkpoints/model_144/tokenizer_config.json\u001b[0m\n",
      "\u001b[32m[2022-07-18 16:28:50,557] [    INFO]\u001b[0m - Special tokens file saved in checkpoints/model_144/special_tokens_map.json\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "def do_train(model,train_data_loader,**kwargs):\n",
    "    save_dir=kwargs['save_dir']\n",
    "    global_step = 0\n",
    "    tic_train = time.time()\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        for step, batch in enumerate(train_data_loader, start=1):\n",
    "            query_input_ids, query_token_type_ids, title_input_ids, title_token_type_ids = batch\n",
    "            # sample的方式使用同义词语句和WR策略\n",
    "            # 概率可以设置\n",
    "            if(random.random()<0.2):\n",
    "                title_input_ids,title_token_type_ids=query_input_ids,query_token_type_ids\n",
    "                query_input_ids,query_token_type_ids=word_repetition(query_input_ids,query_token_type_ids,dup_rate)\n",
    "                title_input_ids,title_token_type_ids=word_repetition(title_input_ids,title_token_type_ids,dup_rate)\n",
    "            # else:\n",
    "            #     query_input_ids,query_token_type_ids=word_repetition(query_input_ids,query_token_type_ids,dup_rate)\n",
    "            #     title_input_ids,title_token_type_ids=word_repetition(title_input_ids,title_token_type_ids,dup_rate)\n",
    "\n",
    "            loss, kl_loss = model(\n",
    "                query_input_ids=query_input_ids,\n",
    "                title_input_ids=title_input_ids,\n",
    "                query_token_type_ids=query_token_type_ids,\n",
    "                title_token_type_ids=title_token_type_ids)\n",
    "            # 加入R-Drop的损失优化，默认设置的是0.1，参数可以调\n",
    "            loss = loss + kl_loss * 0.1\n",
    "            # 每隔5个step打印日志\n",
    "            global_step += 1\n",
    "            if global_step % 5 == 0:\n",
    "                print(\n",
    "                    \"global step %d, epoch: %d, batch: %d, loss: %.5f, speed: %.2f step/s\"\n",
    "                    % (global_step, epoch, step, loss,\n",
    "                       10 / (time.time() - tic_train)))\n",
    "                tic_train = time.time()\n",
    "            # 反向梯度求导更新\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            lr_scheduler.step()\n",
    "            optimizer.clear_grad()\n",
    "            # 每隔save_steps保存模型\n",
    "            if global_step % save_steps == 0:\n",
    "                save_path = os.path.join(save_dir, \"model_%d\" % global_step)\n",
    "                if not os.path.exists(save_path):\n",
    "                    os.makedirs(save_path)\n",
    "                save_param_path = os.path.join(save_path, 'model_state.pdparams')\n",
    "                paddle.save(model.state_dict(), save_param_path)\n",
    "                tokenizer.save_pretrained(save_path)\n",
    "    # 保存最后一个batch的模型\n",
    "    save_path = os.path.join(save_dir, \"model_%d\" % global_step)\n",
    "    if not os.path.exists(save_path):\n",
    "        os.makedirs(save_path)\n",
    "        save_param_path = os.path.join(save_path, 'model_state.pdparams')\n",
    "        paddle.save(model.state_dict(), save_param_path)\n",
    "        tokenizer.save_pretrained(save_path)\n",
    "# 模型训练\n",
    "do_train(model,train_data_loader,save_dir=save_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. 效果评估\n",
    "\n",
    "评估过程首先加载召回集corpus.csv，然后抽取向量，插入到hnswlib索引引擎中，然后用测试集的每个query去hnswlib检索，得到返回结果后计算Recall@N。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-18T06:47:11.120426Z",
     "iopub.status.busy": "2022-07-18T06:47:11.119340Z",
     "iopub.status.idle": "2022-07-18T06:47:11.129358Z",
     "shell.execute_reply": "2022-07-18T06:47:11.128604Z",
     "shell.execute_reply.started": "2022-07-18T06:47:11.120387Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{0: '如何办理企业养老保险'}, {1: '如何为西班牙购买签证保险？'}, {2: '康慧宝需要买多少？'}, {3: '如果另一方对车辆事故负有全部责任，并且拒绝提前支付维修费，该怎么办'}]\n"
     ]
    }
   ],
   "source": [
    "from data import gen_id2corpus\n",
    "corpus_file = 'databaoxian/corpus.csv'\n",
    "id2corpus = gen_id2corpus(corpus_file)\n",
    "# conver_example function's input must be dict\n",
    "corpus_list = [{idx: text} for idx, text in id2corpus.items()]\n",
    "print(corpus_list[:4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-18T06:47:23.381824Z",
     "iopub.status.busy": "2022-07-18T06:47:23.380988Z",
     "iopub.status.idle": "2022-07-18T06:47:23.491709Z",
     "shell.execute_reply": "2022-07-18T06:47:23.456868Z",
     "shell.execute_reply.started": "2022-07-18T06:47:23.381785Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Tensor(shape=[64, 45], dtype=int64, place=Place(cpu), stop_gradient=True,\n",
      "       [[1   , 142 , 449 , ..., 0   , 0   , 0   ],\n",
      "        [1   , 142 , 449 , ..., 0   , 0   , 0   ],\n",
      "        [1   , 736 , 1497, ..., 0   , 0   , 0   ],\n",
      "        ...,\n",
      "        [1   , 530 , 211 , ..., 0   , 0   , 0   ],\n",
      "        [1   , 189 , 31  , ..., 0   , 0   , 0   ],\n",
      "        [1   , 75  , 52  , ..., 0   , 0   , 0   ]]), Tensor(shape=[64, 45], dtype=int64, place=Place(cpu), stop_gradient=True,\n",
      "       [[0, 0, 0, ..., 0, 0, 0],\n",
      "        [0, 0, 0, ..., 0, 0, 0],\n",
      "        [0, 0, 0, ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0, ..., 0, 0, 0],\n",
      "        [0, 0, 0, ..., 0, 0, 0],\n",
      "        [0, 0, 0, ..., 0, 0, 0]])]\n"
     ]
    }
   ],
   "source": [
    "from data import convert_example_test\n",
    "\n",
    "trans_func_corpus = partial(\n",
    "        convert_example_test,\n",
    "        tokenizer=tokenizer,\n",
    "        max_seq_length=max_seq_length)\n",
    "batchify_fn_corpus = lambda samples, fn=Tuple(\n",
    "        Pad(axis=0, pad_val=tokenizer.pad_token_id),  # text_input\n",
    "        Pad(axis=0, pad_val=tokenizer.pad_token_type_id),  # text_segment\n",
    "    ): [data for data in fn(samples)]\n",
    "corpus_ds = MapDataset(corpus_list)\n",
    "corpus_data_loader = create_dataloader(\n",
    "        corpus_ds,\n",
    "        mode='predict',\n",
    "        batch_size=batch_size,\n",
    "        batchify_fn=batchify_fn_corpus,\n",
    "        trans_fn=trans_func_corpus)\n",
    "for item in corpus_data_loader:\n",
    "    print(item)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上图显示的是预测数据的id的形式，第一个Tensor表示的是input_ids，第二个Tensor表示的是Token_type_ids。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用hnswlib来构建索引库。\n",
    "\n",
    "![](https://ai-studio-static-online.cdn.bcebos.com/7948ee8ce0fc47a6a44008d0c902a0ea45c76ec5d39940a38d0d632291db5741)\n",
    "\n",
    "支持三种距离计算的方式，本项目使用的是ip，内积的方式\n",
    "\n",
    "更多参数设置信息可以参考链接：https://github.com/nmslib/hnswlib。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-18T06:47:38.117460Z",
     "iopub.status.busy": "2022-07-18T06:47:38.116688Z",
     "iopub.status.idle": "2022-07-18T06:47:41.664675Z",
     "shell.execute_reply": "2022-07-18T06:47:41.663835Z",
     "shell.execute_reply.started": "2022-07-18T06:47:38.117410Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'hnswlib'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_19121/2473278377.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mann_util\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbuild_index\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m# 索引的大小\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mhnsw_max_elements\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000000\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# 控制时间和精度的平衡参数\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mhnsw_ef\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/实训任务 28/工程文件/ann_util.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mhnswlib\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpaddlenlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlogger\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'hnswlib'"
     ]
    }
   ],
   "source": [
    "from ann_util import build_index\n",
    "# 索引的大小\n",
    "hnsw_max_elements=1000000\n",
    "# 控制时间和精度的平衡参数\n",
    "hnsw_ef=100\n",
    "hnsw_m=100\n",
    "\n",
    "final_index = build_index(corpus_data_loader, model,output_emb_size=output_emb_size,hnsw_max_elements=hnsw_max_elements,\n",
    "                hnsw_ef=hnsw_ef,\n",
    "                hnsw_m=hnsw_m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-18T06:47:48.532654Z",
     "iopub.status.busy": "2022-07-18T06:47:48.531660Z",
     "iopub.status.idle": "2022-07-18T06:47:48.542209Z",
     "shell.execute_reply": "2022-07-18T06:47:48.541311Z",
     "shell.execute_reply.started": "2022-07-18T06:47:48.532616Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'text': '企业养老保险自己怎么办理'}, {'text': '西*牙签证保险怎么买？'}]\n"
     ]
    }
   ],
   "source": [
    "def gen_text_file(similar_text_pair_file):\n",
    "    text2similar_text = {}\n",
    "    texts = []\n",
    "    with open(similar_text_pair_file, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            splited_line = line.rstrip().split(\"\\t\")\n",
    "            if len(splited_line) != 2:\n",
    "                continue\n",
    "\n",
    "            text, similar_text = line.rstrip().split(\"\\t\")\n",
    "\n",
    "            if not text or not similar_text:\n",
    "                continue\n",
    "\n",
    "            text2similar_text[text] = similar_text\n",
    "            texts.append({\"text\": text})\n",
    "    return texts, text2similar_text\n",
    "\n",
    "similar_text_pair_file='baoxian/test_pair.csv'  \n",
    "text_list, text2similar_text = gen_text_file(similar_text_pair_file)\n",
    "print(text_list[:2])\n",
    "# print(text2similar_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-18T06:47:55.116613Z",
     "iopub.status.busy": "2022-07-18T06:47:55.115595Z",
     "iopub.status.idle": "2022-07-18T06:47:55.121228Z",
     "shell.execute_reply": "2022-07-18T06:47:55.120429Z",
     "shell.execute_reply.started": "2022-07-18T06:47:55.116575Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "query_ds = MapDataset(text_list)\n",
    "query_data_loader = create_dataloader(\n",
    "        query_ds,\n",
    "        mode='predict',\n",
    "        batch_size=batch_size,\n",
    "        batchify_fn=batchify_fn_corpus,\n",
    "        trans_fn=trans_func_corpus)\n",
    "query_embedding = model.get_semantic_embedding(query_data_loader)\n",
    "recall_result_dir='recall_result_dir'\n",
    "os.makedirs(recall_result_dir,exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-18T06:47:59.808288Z",
     "iopub.status.busy": "2022-07-18T06:47:59.807705Z",
     "iopub.status.idle": "2022-07-18T06:48:00.710578Z",
     "shell.execute_reply": "2022-07-18T06:48:00.709595Z",
     "shell.execute_reply.started": "2022-07-18T06:47:59.808253Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "recall_num = 10\n",
    "recall_result_file = 'recall_result.txt'\n",
    "recall_result_file = os.path.join(recall_result_dir,\n",
    "                                      recall_result_file)\n",
    "with open(recall_result_file, 'w', encoding='utf-8') as f:\n",
    "    for batch_index, batch_query_embedding in enumerate(query_embedding):\n",
    "        recalled_idx, cosine_sims = final_index.knn_query(\n",
    "            batch_query_embedding.numpy(), recall_num)\n",
    "        batch_size = len(cosine_sims)\n",
    "        for row_index in range(batch_size):\n",
    "            text_index = batch_size * batch_index + row_index\n",
    "            for idx, doc_idx in enumerate(recalled_idx[row_index]):\n",
    "                f.write(\"{}\\t{}\\t{}\\n\".format(text_list[text_index][\n",
    "                        \"text\"], id2corpus[doc_idx], 1.0 - cosine_sims[\n",
    "                            row_index][idx]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-18T06:48:06.794548Z",
     "iopub.status.busy": "2022-07-18T06:48:06.793886Z",
     "iopub.status.idle": "2022-07-18T06:48:06.836608Z",
     "shell.execute_reply": "2022-07-18T06:48:06.835756Z",
     "shell.execute_reply.started": "2022-07-18T06:48:06.794511Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "recall@1=80.053\n",
      "recall@5=89.432\n",
      "recall@10=90.489\n"
     ]
    }
   ],
   "source": [
    "recall_N = []\n",
    "from evaluate import recall\n",
    "from data import get_rs\n",
    "similar_text_pair=\"baoxian/test_pair.csv\"\n",
    "rs=get_rs(similar_text_pair,recall_result_file,10)\n",
    "recall_num = [1, 5, 10]\n",
    "for topN in recall_num:\n",
    "    R = round(100 * recall(rs, N=topN), 3)\n",
    "    recall_N.append(str(R))\n",
    "for key, val in zip(recall_num, recall_N):\n",
    "    print('recall@{}={}'.format(key, val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9. 模型推理\n",
    "\n",
    "取出一条文本数据，模型预测得到向量后，利用hnswlib进行向量检索，得到候选的问题。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-18T06:48:18.357363Z",
     "iopub.status.busy": "2022-07-18T06:48:18.356556Z",
     "iopub.status.idle": "2022-07-18T06:48:18.405255Z",
     "shell.execute_reply": "2022-07-18T06:48:18.404382Z",
     "shell.execute_reply.started": "2022-07-18T06:48:18.357324Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "输入文本：买了社保，是不是就不用买商业保险了？\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'final_index' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_19121/2311655838.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mcls_embedding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_pooled_embedding\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtoken_type_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtoken_type_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# print('提取特征:{}'.format(cls_embedding))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m recalled_idx, cosine_sims = final_index.knn_query(\n\u001b[0m\u001b[1;32m     13\u001b[0m             cls_embedding.numpy(), 10)\n\u001b[1;32m     14\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'检索召回'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'final_index' is not defined"
     ]
    }
   ],
   "source": [
    "example=\"买了社保，是不是就不用买商业保险了？\"\n",
    "print('输入文本：{}'.format(example))\n",
    "encoded_inputs = tokenizer(\n",
    "            text=[example],\n",
    "            max_seq_len=max_seq_length)\n",
    "input_ids = encoded_inputs[\"input_ids\"]\n",
    "token_type_ids = encoded_inputs[\"token_type_ids\"]\n",
    "input_ids = paddle.to_tensor(input_ids)\n",
    "token_type_ids = paddle.to_tensor(token_type_ids)\n",
    "cls_embedding=model.get_pooled_embedding( input_ids=input_ids,token_type_ids=token_type_ids)\n",
    "# print('提取特征:{}'.format(cls_embedding))\n",
    "recalled_idx, cosine_sims = final_index.knn_query(\n",
    "            cls_embedding.numpy(), 10)\n",
    "print('检索召回')\n",
    "for doc_idx,cosine_sim in zip(recalled_idx[0],cosine_sims[0]):\n",
    "    print(id2corpus[doc_idx],cosine_sim)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "输入的文本是：\"买了社保，是不是就不用买商业保险了？\"，经过向量检索，返回了10个候选的问题，其中第一个问题跟输入文本非常接近，说明得到了正确的召回。\n",
    "\n",
    "下一步就把召回的第一条数据的答案返回给用户：\n",
    "\n",
    "```\n",
    "社保是基础的，就是我们通常说的“五险”包括：基本养老保险、基本医疗保险、失业保险、工伤保险和生育保险。而商业保险则是保障。\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10 预测部署\n",
    "\n",
    "预测部署首先需要把动态图模型转换成静态图，然后基于Mivus构建近似向量检索引擎，向Mivus的索引库中插入语料的向量，最后把抽取向量用PaddleServing部署，使得线上的文本都能够从paddleServing抽取向量。\n",
    "下面为大家展示部署的几个关键步骤：\n",
    "\n",
    "## 10.1 动转静导出\n",
    "\n",
    "首先把模型转换成静态图模型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-18T06:48:48.720339Z",
     "iopub.status.busy": "2022-07-18T06:48:48.719340Z",
     "iopub.status.idle": "2022-07-18T06:49:02.859584Z",
     "shell.execute_reply": "2022-07-18T06:49:02.858595Z",
     "shell.execute_reply.started": "2022-07-18T06:48:48.720304Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "output_path='output'\n",
    "# 切换成eval模式，关闭dropout\n",
    "model.eval()\n",
    "# Convert to static graph with specific input description\n",
    "model = paddle.jit.to_static(\n",
    "        model,\n",
    "        input_spec=[\n",
    "            paddle.static.InputSpec(\n",
    "                shape=[None, None], dtype=\"int64\"),  # input_ids\n",
    "            paddle.static.InputSpec(\n",
    "                shape=[None, None], dtype=\"int64\")  # segment_ids\n",
    "        ])\n",
    "# Save in static graph model.\n",
    "save_path = os.path.join(output_path, \"inference\")\n",
    "paddle.jit.save(model, save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.2 问答检索引擎\n",
    "\n",
    "模型准备结束以后，开始搭建 Milvus 的语义检索引擎，用于语义向量的快速检索，本项目使用[Milvus](https://milvus.io/)开源工具进行向量检索，Milvus 的搭建教程请参考官方教程  [Milvus官方安装教程](https://milvus.io/cn/docs/v1.1.1/milvus_docker-cpu.md)本案例使用的是 Milvus 的1.1.1 CPU版本，建议使用官方的 Docker 安装方式，简单快捷。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.3 Paddle Serving 部署\n",
    "\n",
    "使用Pipeline的方式进行部署。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "dirname=\"output\"\n",
    "# 模型的路径\n",
    "model_filename=\"inference.get_pooled_embedding.pdmodel\"\n",
    "# 参数的路径\n",
    "params_filename=\"inference.get_pooled_embedding.pdiparams\" \n",
    "# server的保存地址\n",
    "server_path=\"serving_server\"\n",
    "# client的保存地址\n",
    "client_path=\"serving_client\"\n",
    "# 指定输出的别名\n",
    "feed_alias_names=None\n",
    "# 制定输入的别名\n",
    "fetch_alias_names=\"output_embedding\"\n",
    "# 设置为True会显示日志\n",
    "show_proto=False\n",
    "serving_io.inference_model_to_serving(\n",
    "        dirname=dirname,\n",
    "        serving_server=server_path,\n",
    "        serving_client=client_path,\n",
    "        model_filename=model_filename,\n",
    "        params_filename=params_filename,\n",
    "        show_proto=show_proto,\n",
    "        feed_alias_names=feed_alias_names,\n",
    "        fetch_alias_names=fetch_alias_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "搭建结束以后，就可以启动server部署服务，使用client端访问server端就行了。具体细节参考代码：[https://github.com/PaddlePaddle/PaddleNLP/tree/develop/applications/question_answering/faq_finance/deploy/python](https://github.com/PaddlePaddle/PaddleNLP/tree/develop/applications/question_answering/faq_finance/deploy/python)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "!rm -rf output/\n",
    "# !rm -rf checkpoints/\n",
    "!rm -rf serving_server/\n",
    "!rm -rf serving_client/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 11 参考文献\n",
    "\n",
    "[1] Gao, Tianyu, Xingcheng Yao, and Danqi Chen. “SimCSE: Simple Contrastive Learning of Sentence Embeddings.” ArXiv:2104.08821 [Cs], April 18, 2021. http://arxiv.org/abs/2104.08821.\n",
    "\n",
    "[2] Wu, Xing, et al. \"ESimCSE: Enhanced Sample Building Method for Contrastive Learning of Unsupervised Sentence Embedding.\" arXiv preprint arXiv:2109.04380 (2021). https://arxiv.org/abs/2109.04380.\n",
    "\n",
    "[3] Liang, Xiaobo, Lijun Wu, Juntao Li, Yue Wang, Qi Meng, Tao Qin, Wei Chen, Min Zhang, and Tie-Yan Liu. “R-Drop: Regularized Dropout for Neural Networks.” ArXiv:2106.14448 [Cs], June 28, 2021. http://arxiv.org/abs/2106.14448.  \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_paddlenlp",
   "language": "python",
   "name": "my_paddlenlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "b687cb092f79ef9fa0b54bf175363be94a7fc81ba37b40cae783a41b0aa2cad9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
